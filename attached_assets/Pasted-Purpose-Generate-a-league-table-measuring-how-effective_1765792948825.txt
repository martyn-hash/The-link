Purpose: Generate a league table measuring how effectively project assignees keep projects on schedule, normalised for workload, using a 0–100 score where higher is better.

CONTEXT

You are analysing operational performance within The Link.

We want to evaluate project assignees (individual users assigned to projects), not service owners.

The output must allow filtering by:

Service (e.g. “Bookkeeping”)

Date range (e.g. 1 Nov – 30 Nov)

The objective is to fairly compare staff effectiveness at keeping projects moving on schedule, without advantaging or disadvantaging users based on how many projects they are assigned.

DEFINITIONS

A project is considered “behind schedule” when it exceeds the maximum allowed time in its current stage.

A late event occurs when a project transitions from “on schedule” to “behind schedule”.

Late days are the total number of calendar days a project remains behind schedule within the selected date range.

Projects are attributed to the assignee responsible at the time the lateness occurs.

METRICS (PER ASSIGNEE)

For the selected service and date range, calculate the following per assignee:

Project Count

Number of projects assigned to the user during the period.

Late Events

Total count of times their assigned projects became behind schedule.

Total Late Days

Total days their assigned projects spent behind schedule.

NORMALISED RATES

To ensure fairness regardless of workload size:

Late Incidence Rate (LIR)

LIR = late_events / project_count


Late Duration Rate (LDR)

LDR = total_late_days / project_count

RAW PERFORMANCE SCORE (LOWER = WORSE)

Compute a weighted composite score:

raw_score =
(0.6 × LIR)
+ (0.4 × LDR)


Rules:

Do not round intermediate values

Store raw_score as a decimal value

Lower raw_score means better performance

NORMALISATION TO 0–100 INDEX (OPTION A)

Convert raw scores into a relative performance index where higher is better, using the worst performer in the cohort as the baseline.

Steps:

Identify:

max_raw_score = MAX(raw_score)


across all assignees in the selected service and date range.

Convert to index:

performance_score =
100 × (1 − (raw_score / max_raw_score))


Apply constraints:

Minimum score = 0

Maximum score = 100

Round only for presentation (not storage)

OUTPUT REQUIREMENTS

Produce a league table ordered by performance_score DESC.

Each row must include:

Rank

Assignee name

Service name

Date range

Project count

Late incidence rate (LIR)

Late duration rate (LDR)

Performance score (0–100)

INTERPRETATION NOTES (FOR UI / DOCUMENTATION)

Scores are relative to peers within the selected service and date range.

A higher score indicates better control of workflow, not lower workload.

Scores are intended for internal analysis and coaching, not punitive evaluation.

Do not compare scores across different services or date ranges without recalculation.

OPTIONAL SAFETY RULES (IF DATA IS THIN)

Exclude assignees with fewer than X projects (e.g. < 10) to reduce statistical noise.

Flag small sample sizes clearly if included.

FINAL INSTRUCTION

Implement this methodology exactly as described.
Do not introduce additional weighting, caps, or smoothing unless explicitly instructed.

If you want next iterations later, the natural extensions are:

Trend scoring (period-on-period delta)

Severity bands

Capacity-adjusted expected lateness